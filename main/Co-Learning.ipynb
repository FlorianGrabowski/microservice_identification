{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from util.graph_util import GraphUtil\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = \"../data/JavaFX-Point-of-Sales_pdg.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(filename, \"r\") as f:\n",
    "    data = json.loads(f.read())\n",
    "entities = data['pdg']['entities']\n",
    "relations = data['pdg']['relations']\n",
    "\n",
    "entities_dict = {entity['entityId']: entity for entity in entities}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(entities))\n",
    "print(len(relations))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gu = GraphUtil(\"PYTHON_2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entities_by_type = gu.get_entities_by_type(entities)\n",
    "print(entities_by_type.keys())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(entities_by_type['CLASS']))\n",
    "print(len(entities_by_type['INTERFACE']))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MG = gu.build_program_dependency_graph(\n",
    "                                    entities,\n",
    "                                    relations,\n",
    "                                    allowed_entity_types=[],\n",
    "                                    allowed_relation_types=['CALL'],\n",
    "                                    entity_attributes=[gu.ENTITY_ID, gu.ENTITY_TYPE, gu.ENTITY_FQN],\n",
    "                                    directed=True,\n",
    "                                    isolated_nodes=True,\n",
    "                                    self_loops=False,\n",
    "                                    weight=\"weight\",\n",
    "                                    multi_edges=True)\n",
    "\n",
    "MG_ORM = gu.build_program_dependency_graph(\n",
    "                                    entities,\n",
    "                                    relations,\n",
    "                                    allowed_entity_types=[],\n",
    "                                    allowed_relation_types=['ORM'],\n",
    "                                    entity_attributes=[gu.ENTITY_ID, gu.ENTITY_TYPE, gu.ENTITY_FQN],\n",
    "                                    directed=True,\n",
    "                                    isolated_nodes=True,\n",
    "                                    self_loops=False,\n",
    "                                    weight=\"weight\",\n",
    "                                    multi_edges=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(MG)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entities_cc:dict = {}\n",
    "for entity in entities:\n",
    "    max_cc = 0\n",
    "    for method in entity['methods']:\n",
    "        metric_container = method['metric_container']\n",
    "        cc = 0\n",
    "        for metric in metric_container:\n",
    "            if metric['name'] == 'CyclomaticComplexity':\n",
    "                cc = metric['value']\n",
    "\n",
    "        if cc > max_cc:\n",
    "            max_cc = cc\n",
    "    entities_cc[entity['entityId']] = max_cc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(entities_cc)\n",
    "plt.hist(entities_cc.values())\n",
    "plt.xlabel('cyclomatic complexity')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### high in_degree --> high fan-in; high out_degree --> high fan-out\n",
    "\n",
    "in_degree = sorted([(entity_id, degree) for  (entity_id, degree) in MG.in_degree(weight=\"weight\")], key=lambda x:x[1], reverse=True)\n",
    "out_degree = sorted([(entity_id, degree) for  (entity_id, degree) in MG.out_degree(weight=\"weight\")], key=lambda x:x[1], reverse=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(in_degree)\n",
    "plt.hist(list(o[1] for o in in_degree))\n",
    "plt.xlabel('in degree')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n",
    "print(out_degree)\n",
    "plt.hist(list(o[1] for o in out_degree))\n",
    "plt.xlabel('out degree')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_high_in_degree = 10#15\n",
    "_very_high_in_degree = 20#200\n",
    "_low_out_degree = 3#3\n",
    "_low_in_degree = 2#4\n",
    "_high_c_complexity = 3#4\n",
    "\n",
    "allowed_types = ['CLASS', 'INTERFACE']\n",
    "\n",
    "# not persistent == no ORM relation\n",
    "\n",
    "allowed_types_set = set()\n",
    "for(entity_id) in entities_dict:\n",
    "    if entities_dict[entity_id]['entityType'] in allowed_types:\n",
    "        allowed_types_set.add(entity_id)\n",
    "print('allowed_types_set: ' + repr(allowed_types_set))\n",
    "\n",
    "high_in_degree = set()\n",
    "for (entity_id, degree) in in_degree:\n",
    "    if degree > _high_in_degree:\n",
    "        high_in_degree.add(entity_id)\n",
    "high_in_degree.intersection_update(allowed_types_set)\n",
    "print('high_in_degree: ' + repr(high_in_degree))\n",
    "\n",
    "very_high_in_degree = set()\n",
    "for (entity_id, degree) in in_degree:\n",
    "    if degree > _very_high_in_degree:\n",
    "        very_high_in_degree.add(entity_id)\n",
    "very_high_in_degree.intersection_update(allowed_types_set)\n",
    "print('very_high_in_degree: ' + repr(very_high_in_degree))\n",
    "\n",
    "persistent_set = set()\n",
    "for(entity_id) in entities_dict:\n",
    "    if len(MG_ORM.edges(entity_id, data=True)) > 0:\n",
    "        persistent_set.add(entity_id)\n",
    "persistent_set.intersection_update(allowed_types_set)\n",
    "print('persistent_set: ' + repr(persistent_set))\n",
    "print('not_persistent_set: ' + repr(allowed_types_set.difference(persistent_set)))\n",
    "\n",
    "low_out_degree = set()\n",
    "for (entity_id, degree) in out_degree:\n",
    "    if degree < _low_out_degree:\n",
    "        low_out_degree.add(entity_id)\n",
    "low_out_degree.intersection_update(allowed_types_set)\n",
    "print('low_out_degree: ' + repr(low_out_degree))\n",
    "\n",
    "low_in_degree = set()\n",
    "for (entity_id, degree) in in_degree:\n",
    "    if degree < _low_in_degree:\n",
    "        low_in_degree.add(entity_id)\n",
    "low_in_degree.intersection_update(allowed_types_set)\n",
    "print('low_in_degree: ' + repr(low_in_degree))\n",
    "\n",
    "err_handling_set = set()\n",
    "for(entity_id) in entities_dict:\n",
    "    for method in entities_dict.get(entity_id)['methods']:\n",
    "        if 'catch' in method['body']:\n",
    "            err_handling_set.add(entity_id)\n",
    "            break\n",
    "err_handling_set.intersection_update(allowed_types_set)\n",
    "print('err_handling_set: ' + repr(err_handling_set))\n",
    "\n",
    "high_c_complexity = set()\n",
    "for (entity_id) in entities_cc:\n",
    "    if entities_cc.get(entity_id) > _high_c_complexity:\n",
    "        high_c_complexity.add(entity_id)\n",
    "high_c_complexity.intersection_update(allowed_types_set)\n",
    "print('high_c_complexity: ' + repr(high_c_complexity))\n",
    "\n",
    "additional_util_set = set()\n",
    "for (entity_id) in entities_dict:\n",
    "    if 'lombok.experimental.UtilityClass' in entities_dict.get(entity_id)['imports']:\n",
    "        additional_util_set.add(entity_id)\n",
    "additional_util_set.intersection_update(allowed_types_set)\n",
    "print('additional_util_set: ' + repr(additional_util_set))\n",
    "print(len(persistent_set))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utility_services = very_high_in_degree.intersection(low_out_degree).difference(persistent_set).union(additional_util_set)#.union({54}) #Very High Fan-in AND Very Low Fanout AND Not persistent\n",
    "for entityId in utility_services:\n",
    "    print(entityId, entities_dict[entityId]['entityFullQualifiedName'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Not Utility service AND High Fan-in AND Low Fanout AND Persistent AND Access to infrastructure AND Fine grained\n",
    "persistentEntity_services:set = high_in_degree.difference(utility_services).intersection(low_out_degree).intersection(persistent_set)\n",
    "\n",
    "for entityId in persistentEntity_services:\n",
    "    print(entityId, entities_dict[entityId]['entityFullQualifiedName'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "entity_names = []\n",
    "for entity_id in persistentEntity_services:\n",
    "    entity_names.append(str(entities_dict[entity_id]['entityFullQualifiedName']))\n",
    "\n",
    "calls_entity_set = set()\n",
    "for entity_id in allowed_types_set:\n",
    "    if len(entities_dict.get(entity_id)['methods']) == 0: continue\n",
    "    for method in entities_dict.get(entity_id)['methods']:\n",
    "        if len(method['invocations']) == 0: continue\n",
    "        for all_calls in method['invocations']:\n",
    "            for persistentEntity_call in entity_names:\n",
    "                if persistentEntity_call in all_calls:\n",
    "                    calls_entity_set.add(entity_id)\n",
    "                    break\n",
    "print('calls_entity_set: ' + repr(calls_entity_set))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Not Utility AND Not Entity AND Low Fan-in AND ( Call to Entity ≥1 OR High CComplexity OR Error Handling)\n",
    "application_services:set = calls_entity_set.union(high_c_complexity).union(err_handling_set).intersection(low_in_degree).difference(persistentEntity_services).difference(utility_services)#.difference({54})\n",
    "for entityId in application_services:\n",
    "    print(entityId, entities_dict[entityId]['entityFullQualifiedName'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print( 'Percentage of labeled ' + repr(allowed_types) + ': ' + repr((len(application_services.union(utility_services).union(persistentEntity_services)) / len(allowed_types_set)) * 100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create second Vew for Co-Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "second_vew_dict_dict:dict = {}\n",
    "for _entity in allowed_types_set:\n",
    "    _vew_list:list = [next(iter([i[1] for i in in_degree if i[0] == _entity])),\n",
    "                      next(iter([i[1] for i in out_degree if i[0] == _entity])),\n",
    "                      entities_cc.get(_entity)]\n",
    "    _err = 0\n",
    "    for(entity_id) in entities_dict:\n",
    "        for method in entities_dict.get(entity_id)['methods']:\n",
    "            if 'catch' in method['body']:\n",
    "                _err = _err + 1\n",
    "    _vew_list.append(_err)\n",
    "    _pers = 0\n",
    "    for(entity_id) in entities_dict:\n",
    "        if len(MG_ORM.edges(entity_id, data=True)) > 0:\n",
    "            _pers = 1\n",
    "    _vew_list.append(_pers)\n",
    "    _vew_list.append(entities_dict[_entity]['loc'])\n",
    "    second_vew_dict_dict[_entity] = np.array(_vew_list)\n",
    "print(second_vew_dict_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get Entity Wording"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "_filtered_words:list = ['id','count','name','date']\n",
    "_list_persistent_words:list = []\n",
    "for(entity_id) in entities_dict:\n",
    "    if entities_dict[entity_id]['entityType'] == \"TABLE\":\n",
    "        _list_persistent_words.extend(entities_dict[entity_id]['fields'])\n",
    "    #print(_list_persistent_words)\n",
    "    for word in _list_persistent_words:\n",
    "        del _list_persistent_words[_list_persistent_words.index(word)]\n",
    "        _list_persistent_words.extend(re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', word)).split())\n",
    "_list_persistent_words = [x.lower() for x in _list_persistent_words]\n",
    "s:set = set(_filtered_words)\n",
    "temp3 = [x for x in _list_persistent_words if x not in s]\n",
    "_persistent_word_dict = {i: temp3.count(i) for i in temp3}\n",
    "print(_persistent_word_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regeneration of code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_position_invert(code:int, pos:int)-> bool:\n",
    "    mask = 1 << pos\n",
    "    return not (code&mask) == mask\n",
    "\n",
    "def get_method(method_dict, **kwargs) -> str:\n",
    "    #optional kwargs to deactivate features can be passed like 'annotations'=False\n",
    "    annotations = kwargs.get('annotations',True)\n",
    "    annotation_arguments = kwargs.get('annotation_arguments',True)\n",
    "    method_head = kwargs.get('method_head',True)\n",
    "    method_modifiers = kwargs.get('method_modifiers',True)\n",
    "    return_types = kwargs.get('returnTypes',True)\n",
    "    method_parameters = kwargs.get('method_parameters',True)\n",
    "    parameter_annotations = kwargs.get('parameter_annotations',True)\n",
    "    parameter_types = kwargs.get('parameter_types',True)\n",
    "    parameter_names = kwargs.get('parameter_names',True)\n",
    "    method_body = kwargs.get('method_body',True)\n",
    "    comments = kwargs.get('comments',True)\n",
    "    method_java_doc = kwargs.get('method_java_doc',True)\n",
    "\n",
    "    methodText:str = ''\n",
    "    #annotations\n",
    "    if method_java_doc:\n",
    "        methodText += method_dict['javaDoc']\n",
    "    if annotations:\n",
    "        for annotation in method_dict['annotations']:\n",
    "            methodText += annotation['type'] + ' '\n",
    "            if annotation_arguments:\n",
    "                for argument in annotation['arguments']:\n",
    "                    methodText = methodText + str(argument) + ' '\n",
    "            methodText += '\\n'\n",
    "    #method head\n",
    "    if method_head:\n",
    "        if method_modifiers:\n",
    "            for modifier in method_dict['modifiers']:\n",
    "                methodText += modifier + ' '\n",
    "        if return_types:\n",
    "            for returnType in method_dict['returnTypes']:\n",
    "                methodText += returnType + ' '\n",
    "        if method_parameters:\n",
    "            methodText += method_dict['name'] + '('\n",
    "            if len(method_dict['parameters']) > 0:\n",
    "                for i,parameter in enumerate(method_dict['parameters']):\n",
    "                    if parameter_annotations:\n",
    "                        for annotation in parameter['annotations']:\n",
    "                            if type(annotation[0]) == str:\n",
    "                                methodText += annotation[0] + ' '\n",
    "                            else:\n",
    "                                methodText += annotation['type'] + ' '\n",
    "                                for argument in annotation['arguments']:\n",
    "                                    methodText += str(argument) + ' '\n",
    "                    if parameter_types:\n",
    "                        methodText += parameter['type'] + ' '\n",
    "                    if parameter_names:\n",
    "                        methodText += parameter['name']\n",
    "                    if not i == len(method_dict['parameters'])-1:\n",
    "                        methodText += ', '\n",
    "                    else:\n",
    "                        methodText += ')'\n",
    "            else:\n",
    "                methodText += ')'\n",
    "    #method body\n",
    "    if method_body:\n",
    "        methodText += method_dict['body']\n",
    "    #comments\n",
    "    if comments:\n",
    "        for comment in method_dict['comments']:\n",
    "            methodText += ' ' + comment\n",
    "    methodText += '\\n'\n",
    "    return methodText\n",
    "\n",
    "def get_class(entity_dict, **kwargs) -> str:\n",
    "    inherited_methods = kwargs.get('inherited_methods',True)\n",
    "    package = kwargs.get('package',True)\n",
    "    imports = kwargs.get('imports',True)\n",
    "    comments = kwargs.get('comments',True)\n",
    "    java_doc = kwargs.get('java_doc',True)\n",
    "    instance_head = kwargs.get('instance_head',True)\n",
    "    instance_modifiers = kwargs.get('instance_modifiers',True)\n",
    "    instance_type = kwargs.get('instance_type',True)\n",
    "    instance_name = kwargs.get('instance_name',True)\n",
    "    instance_extension = kwargs.get('instance_extension',True)\n",
    "    instance_implementations = kwargs.get('instance_implementations',True)\n",
    "    fields = kwargs.get('fields',True)\n",
    "    field_annotations = kwargs.get('field_annotations',True)\n",
    "    field_modifiers = kwargs.get('field_modifiers',True)\n",
    "    field_type = kwargs.get('field_type',True)\n",
    "    field_name = kwargs.get('field_name',True)\n",
    "    methods = kwargs.get('methods',True)\n",
    "    method_code = kwargs.get('method_code',0)\n",
    "\n",
    "    entityText:str = ''\n",
    "    #package\n",
    "    if package:\n",
    "        entityText += 'package ' + entity_dict['package'] + '\\n\\n'\n",
    "    #imports\n",
    "    if imports:\n",
    "        for codeImport in entity_dict['imports']:\n",
    "            entityText += 'import ' + codeImport + '\\n'\n",
    "        entityText += '\\n'\n",
    "    #commments\n",
    "    if comments:\n",
    "        for comment in entity_dict['comments']:\n",
    "                entityText += comment + '\\n'\n",
    "    if java_doc:\n",
    "        for doc in entity_dict['javaDoc']:\n",
    "                entityText += doc + '\\n'\n",
    "    #head\n",
    "    if instance_head:\n",
    "        if instance_modifiers:\n",
    "            for modifier in entity_dict['modifiers']:\n",
    "                    entityText += modifier + ' '\n",
    "        if instance_type:\n",
    "            entityText += repr(entity_dict['entityType']).strip('\\'').lower() + ' '\n",
    "        if instance_name:\n",
    "            entityText += entity_dict['entityName'] + ' '\n",
    "        if instance_extension:\n",
    "            if len(entity_dict['extends']) > 0:\n",
    "                entityText += 'extends '\n",
    "            for i, interface in enumerate(entity_dict['extends']):\n",
    "                if not i == len(entity_dict['extends']) - 1:\n",
    "                    entityText += interface + ', '\n",
    "                else:\n",
    "                    entityText += interface + ' '\n",
    "        if instance_implementations:\n",
    "            if len(entity_dict['interfaces']) > 0:\n",
    "                entityText += 'implements '\n",
    "            for i, interface in enumerate(entity_dict['interfaces']):\n",
    "                if not i == len(entity_dict['interfaces']) - 1:\n",
    "                    entityText += interface + ', '\n",
    "                else:\n",
    "                    entityText += interface + ' '\n",
    "        entityText += '{\\n'\n",
    "    #fields\n",
    "    if fields:\n",
    "        for codeField in entity_dict['fields']:\n",
    "            if field_annotations:\n",
    "                for annotation in codeField['annotations']:\n",
    "                        entityText += annotation['type'] + '\\n'\n",
    "            if field_modifiers:\n",
    "                for modifier in codeField['modifiers']:\n",
    "                    entityText += modifier + ' '\n",
    "            if field_type:\n",
    "                entityText += codeField['type'] + ' '\n",
    "            if field_name:\n",
    "                entityText += codeField['name'] + '\\n'\n",
    "        entityText += '\\n'\n",
    "    #mehtods\n",
    "    if methods:\n",
    "        for codeMethod in entity_dict['methods']:\n",
    "            if not codeMethod['inherited_from_superclass'] or inherited_methods: #kick out methods (which are inherited)\n",
    "                entityText += get_method(codeMethod, annotations = get_position_invert(method_code, 0),\n",
    "                                         annotation_arguments = get_position_invert(method_code, 1),\n",
    "                                         method_head = get_position_invert(method_code, 2),\n",
    "                                         method_modifiers = get_position_invert(method_code, 3),\n",
    "                                         return_types = get_position_invert(method_code, 4),\n",
    "                                         method_parameters = get_position_invert(method_code, 5),\n",
    "                                         parameter_annotations = get_position_invert(method_code, 6),\n",
    "                                         parameter_types = get_position_invert(method_code, 7),\n",
    "                                         parameter_names = get_position_invert(method_code, 8),\n",
    "                                         method_body = get_position_invert(method_code, 9),\n",
    "                                         comments = get_position_invert(method_code, 10),\n",
    "                                         method_java_doc = get_position_invert(method_code, 11))\n",
    "    #everything after methods\n",
    "    entityText += '}\\n'\n",
    "    return entityText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup of torch and GPU device testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device: {torch.cuda.current_device()}\")\n",
    "print(f\"Name of current CUDA device: {torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "# Set max_split_size_mb to 512 MB\n",
    "torch.backends.cuda.split_kernel_size = 8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup of codebert model and tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
    "config.output_hidden_states = True #should also be working but does not\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\",config)\n",
    "model.config.output_hidden_states = True\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting all embeddings for the different classified and unclassified entities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPoolingAndCrossAttentions\n",
    "\n",
    "def get_embeddings_class_options(_service, _code_for_class: int, _code_for_method: int) -> torch.tensor:\n",
    "    _embeddings_dict: dict = {}\n",
    "\n",
    "    text = get_class(entities_dict[_service], inherited_methods=get_position_invert(_code_for_class, 0),\n",
    "                     package=get_position_invert(_code_for_class, 1),\n",
    "                     imports=get_position_invert(_code_for_class, 2),\n",
    "                     comments=get_position_invert(_code_for_class, 3),\n",
    "                     java_doc=get_position_invert(_code_for_class, 4),\n",
    "                     instance_head=get_position_invert(_code_for_class, 5),\n",
    "                     instance_modifiers=get_position_invert(_code_for_class, 6),\n",
    "                     instance_type=get_position_invert(_code_for_class, 7),\n",
    "                     instance_name=get_position_invert(_code_for_class, 8),\n",
    "                     instance_extension=get_position_invert(_code_for_class, 9),\n",
    "                     instance_implementations=get_position_invert(_code_for_class, 10),\n",
    "                     fields=get_position_invert(_code_for_class, 11),\n",
    "                     field_annotations=get_position_invert(_code_for_class, 12),\n",
    "                     field_modifiers=get_position_invert(_code_for_class, 13),\n",
    "                     field_type=get_position_invert(_code_for_class, 14),\n",
    "                     field_name=get_position_invert(_code_for_class, 15),\n",
    "                     methods=get_position_invert(_code_for_class, 16),\n",
    "                     method_code=_code_for_method)\n",
    "    #print(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "    chunks, chunk_size = len(tokenized_text), 510\n",
    "    split_tokenized_text: list = [tokenized_text[i:i + chunk_size] for i in range(0, chunks, chunk_size)]\n",
    "    for n, tokenized_text_slice in enumerate(split_tokenized_text):\n",
    "        tokenized_text_slice = [tokenizer.cls_token] + tokenized_text_slice + [tokenizer.sep_token]\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_slice)\n",
    "        segments_ids = [1] * len(tokenized_text_slice)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        tokens_tensor = tokens_tensor.to(device)\n",
    "        segments_tensor = torch.tensor([segments_ids])\n",
    "        segments_tensor = segments_tensor.to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output: BaseModelOutputWithPoolingAndCrossAttentions = model(tokens_tensor, segments_tensor)\n",
    "        _all_hidden_layers_of_slice: dict = {}\n",
    "        _mean: torch.tensor() = torch.zeros(768).to(device)\n",
    "        for i, slice_hidden_state_tensor in enumerate(model_output.hidden_states):\n",
    "            _all_hidden_layers_of_slice[i] = (slice_hidden_state_tensor[-1][-1])\n",
    "            if i < 13:\n",
    "                _mean = torch.add(_mean, slice_hidden_state_tensor[-1][-1])\n",
    "                #print(f\"added {i}\")\n",
    "        _all_hidden_layers_of_slice[13] = _mean #there are 13 regular hidden layers 0-12 and one mean of the above specified located at pos 13\n",
    "        _embeddings_dict[n] = _all_hidden_layers_of_slice\n",
    "    print(f\"Made {len(_embeddings_dict.keys())} embeddings for {entities_dict[_service]['entityFullQualifiedName']} with class code {bin(_code_for_class)} and method code {bin(_code_for_method)}\")\n",
    "    _mean: torch.tensor() = torch.zeros(768).to(device)\n",
    "    for _mean_embedding_of_slice in _embeddings_dict.keys():\n",
    "        _mean = torch.add(_mean, _embeddings_dict.get(_mean_embedding_of_slice).get(13))  # summ all the averages to make an overall class average\n",
    "    return _mean/len(_embeddings_dict)\n",
    "\n",
    "embeddings_list_dict: dict = {}\n",
    "\n",
    "if True:\n",
    "    persistentEntity_services_embeddings: dict = {}\n",
    "    for _service in persistentEntity_services:\n",
    "        persistentEntity_services_embeddings[_service] = get_embeddings_class_options(_service, 0 << 1, 1 << 10)\n",
    "        #print(len(persistentEntity_services_embeddings[_service][0][0]))\n",
    "    application_services_embeddings: dict = {}\n",
    "    for _service in application_services:\n",
    "        application_services_embeddings[_service] = get_embeddings_class_options(_service, 0 << 1, 1 << 10)\n",
    "    utility_services_embeddings: dict = {}\n",
    "    for _service in utility_services:\n",
    "        utility_services_embeddings[_service] = get_embeddings_class_options(_service, 0 << 1, 1 << 10)\n",
    "    unclassified_services_embeddings: dict = {}\n",
    "    for _service in allowed_types_set.difference(utility_services).difference(persistentEntity_services).difference(\n",
    "            application_services):\n",
    "        unclassified_services_embeddings[_service] = get_embeddings_class_options(_service, 0 << 1, 1 << 10)\n",
    "    embeddings_list_dict[(0, 8)] = [persistentEntity_services_embeddings, application_services_embeddings,\n",
    "                                    utility_services_embeddings, unclassified_services_embeddings]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = ['persistentEntity', 'application', 'utility']\n",
    "\n",
    "file_app = open(\"../data/posapplication.txt\", \"r\")\n",
    "file_util = open(\"../data/posutility.txt\", \"r\")\n",
    "file_entity = open(\"../data/posentity.txt\", \"r\")\n",
    "\n",
    "app_text = file_app.read()\n",
    "entity_text = file_entity.read()\n",
    "util_text = file_util.read()\n",
    "\n",
    "_first_code_embeddings_list:list = list(embeddings_list_dict.values())[0]\n",
    "\n",
    "TP_util = 0\n",
    "TP_pers = 0\n",
    "TP_app = 0\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[2]#classified util\n",
    "_number_of_classified_util = len(_first_code_embeddings_list[2])\n",
    "_number_of_actual_util = util_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        TP_util = TP_util + 1\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[0]#classified pers\n",
    "_number_of_classified_pers = len(_first_code_embeddings_list[0])\n",
    "_number_of_actual_pers = entity_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        TP_pers = TP_pers + 1\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[1]#classified app\n",
    "_number_of_classified_app = len(_first_code_embeddings_list[1])\n",
    "_number_of_actual_app = app_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        #print(f\"{name} is {classes[0]}.\")\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        #print(f\"{name} is {classes[1]}.\")\n",
    "        TP_app = TP_app + 1\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        #print(f\"{name} is {classes[2]}.\")\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "if TP_pers + TP_app + TP_util > 0:\n",
    "    FP_util = _number_of_classified_util - TP_util\n",
    "    FN_util = _number_of_actual_util - TP_util\n",
    "    TN_util =  _number_of_actual_app + _number_of_actual_pers - FP_util\n",
    "    FP_pers = _number_of_classified_pers - TP_pers\n",
    "    FN_pers = _number_of_actual_pers - TP_pers\n",
    "    TN_pers = _number_of_actual_app + _number_of_actual_util - FP_pers\n",
    "    FP_app = _number_of_classified_app - TP_app\n",
    "    FN_app = _number_of_actual_app - TP_app\n",
    "    TN_app = _number_of_actual_util + _number_of_actual_pers - FP_app\n",
    "    _accuracy = (TP_util + TP_app + TP_pers + TN_app + TN_util + TN_pers) / (TP_util + TP_app + TP_pers + TN_app + TN_util + TN_pers + FP_app + FP_pers + FP_util + FN_util + FN_app + FN_pers)\n",
    "    print(f\"accuracy: {_accuracy}\")\n",
    "    if not TP_util == 0:\n",
    "        _util_precision = TP_util / (TP_util + FP_util)\n",
    "        _util_recall = TP_util / (TP_util + FN_util)\n",
    "        print(f\"precision util: {_util_precision} | recall util: {_util_recall} | F-1 measure: {2 * ((_util_precision * _util_recall) / (_util_precision + _util_recall))}\")\n",
    "    if not TP_app == 0:\n",
    "        _app_precision = TP_app / (TP_app + FP_app)\n",
    "        _app_recall = TP_app / (TP_app + FN_app)\n",
    "        print(f\"precision application: {_app_precision} | recall application: {_app_recall} | F-1 measure: {2 * ((_app_precision * _app_recall) / (_app_precision + _app_recall))}\")\n",
    "    if not TP_app == 0:\n",
    "        _pers_precision = TP_pers / (TP_pers + FP_pers)\n",
    "        _pers_recall = TP_pers / (TP_pers + FN_pers)\n",
    "        print(f\"precision persistent entity: {_pers_precision} | recall application: {_pers_recall} | F-1 measure: {2 * ((_pers_precision * _pers_recall) / (_pers_precision + _pers_recall))}\")\n",
    "    _total_precision = (TP_util + TP_app + TP_pers) / (TP_util + TP_app + TP_pers + FP_app + FP_util + FP_pers)\n",
    "    _total_recall = (TP_util + TP_app + TP_pers) / (TP_util + TP_app + TP_pers + FN_app + FN_util + FN_pers)\n",
    "    print(f\"precision total: {_total_precision} | recall total: {_total_recall} | F-1 measure total: {2 * ((_total_precision * _total_recall) / (_total_precision + _total_recall))}\")\n",
    "else: print(\"Could not get truth.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Datastructures for sklearn with numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = ['persistentEntity', 'application', 'utility']\n",
    "\n",
    "def move_embeddings_to_numpy() -> dict:\n",
    "    _learn_dict = {}\n",
    "    for _embedding_list_key in embeddings_list_dict.keys():\n",
    "        _X: list = []\n",
    "        _X2: list = []\n",
    "        _y: list = []\n",
    "        _persistentEntity_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[0]\n",
    "        for _persistentEntity_services_embedding_key in _persistentEntity_services_embeddings.keys():\n",
    "            _y.append(classes[0])\n",
    "            _X.append(_persistentEntity_services_embeddings.get(_persistentEntity_services_embedding_key).cpu().detach().numpy())\n",
    "            _X2.append(second_vew_dict_dict.get(_persistentEntity_services_embedding_key))\n",
    "        _application_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[1]\n",
    "        for _application_services_embedding_key in _application_services_embeddings.keys():\n",
    "            _y.append(classes[1])\n",
    "            _X.append(_application_services_embeddings.get(_application_services_embedding_key).cpu().detach().numpy())\n",
    "            _X2.append(second_vew_dict_dict.get(_application_services_embedding_key))\n",
    "        _utility_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[2]\n",
    "        for _utility_services_embedding_key in _utility_services_embeddings.keys():\n",
    "            _y.append(classes[2])\n",
    "            _X.append(_utility_services_embeddings.get(_utility_services_embedding_key).cpu().detach().numpy())\n",
    "            _X2.append(second_vew_dict_dict.get(_utility_services_embedding_key))\n",
    "        _learn_dict[_embedding_list_key] = (np.array(_X), np.array(_y),np.array(_X2))\n",
    "    return _learn_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making a pipeline for SVM and fitting(training)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_duplicates(lst): #removes tuples based on first element occurrence\n",
    "    visited = {}\n",
    "    Output = [] # Output list initialization\n",
    "\n",
    "    for a, b in lst:# Iterate through the list of tuples\n",
    "        if a not in visited:        # Check if the first value is already present in the dictionary\n",
    "            visited[a] = True       # If it is not present, add the key-value pair to the dictionary\n",
    "            Output.append((a, b))   # Append the tuple to the output list\n",
    "    return Output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_clf(kernel:str,X,y,Z):\n",
    "    clf = make_pipeline(StandardScaler(),SVC(C=1.0, kernel=kernel,probability=True))\n",
    "    clf.fit(X, y)\n",
    "    return clf.predict_proba(np.array(Z))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_highest_N(L,n)-> list:\n",
    "    _A:np.array = np.array(L)              #make 2D array of all prediction and their 3 probabilities shape = len(_Z):3\n",
    "    _A_1d:np.array = _A.flatten()           #make 1D array of all probabilities\n",
    "    _tuple_list_highest:list = []           #prepare list to store top N predictions based on probability\n",
    "    for _c_1d in np.flipud(_A_1d.argsort()[-n:]): #getting the top 5 values ->\n",
    "        _tuple_list_highest.append(np.unravel_index(_c_1d, _A.shape)) #making a tuple to get original 2D location and save it to list\n",
    "    return remove_duplicates(_tuple_list_highest) #making sure there is no two same values for D1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for _embedding_list_key in embeddings_list_dict.keys():\n",
    "    _Z = []\n",
    "    _Z2 = []\n",
    "    _unclassified_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[3]\n",
    "    for _unclassified_services_embedding_key in _unclassified_services_embeddings.keys():\n",
    "        _Z.append(_unclassified_services_embeddings.get(_unclassified_services_embedding_key).cpu().detach().numpy())\n",
    "        _Z2.append(second_vew_dict_dict.get(_unclassified_services_embedding_key))\n",
    "    while len(_Z) > 0 and len(_Z2) > 0:\n",
    "        _np_dict = move_embeddings_to_numpy()\n",
    "        _y = _np_dict.get(_embedding_list_key)[1]\n",
    "        _X = _np_dict.get(_embedding_list_key)[0]\n",
    "        _X2 = _np_dict.get(_embedding_list_key)[2]\n",
    "        _L2 = train_clf('linear',_X2,_y,_Z2)\n",
    "        _L = train_clf('poly',_X,_y,_Z) #make predictions with probability\n",
    "        top_1:list = get_highest_N(_L,5)\n",
    "        top_2:list = get_highest_N(_L2,5)\n",
    "        _combined_top_list:list = top_2 + top_1\n",
    "        _tuple_list_highest_cleansed = remove_duplicates(_combined_top_list)\n",
    "        for (list_index, probability_array_index) in sorted(_tuple_list_highest_cleansed,reverse=True): #sorted because of removal order\n",
    "            if (list_index, probability_array_index) in top_1:\n",
    "                _probability_array = _L[list_index]\n",
    "                print(_probability_array)\n",
    "                _probability_value = _probability_array[probability_array_index]\n",
    "            else:\n",
    "                _probability_array = _L2[list_index]\n",
    "                print(_probability_array)\n",
    "                _probability_value = _probability_array[probability_array_index]\n",
    "            if _probability_value < 0.33334:\n",
    "                raise Exception(\"prediction err\")\n",
    "\n",
    "            if probability_array_index == 2:\n",
    "                print(f\"unclassified service: {entities_dict[list(_unclassified_services_embeddings.keys())[list_index]]['entityFullQualifiedName'],list(_unclassified_services_embeddings.keys())[list_index]} was classified:\\x1b[34m {classes[probability_array_index]}\\x1b[0m with probability of: {_probability_value*100}%.\")\n",
    "            elif probability_array_index == 0:\n",
    "                print(f\"unclassified service: {entities_dict[list(_unclassified_services_embeddings.keys())[list_index]]['entityFullQualifiedName'],list(_unclassified_services_embeddings.keys())[list_index]} was classified:\\x1b[31m {classes[probability_array_index]}\\x1b[0m with probability of: {_probability_value*100}%.\")\n",
    "            elif probability_array_index == 1:\n",
    "                print(f\"unclassified service: {entities_dict[list(_unclassified_services_embeddings.keys())[list_index]]['entityFullQualifiedName'],list(_unclassified_services_embeddings.keys())[list_index]} was classified:\\x1b[32m {classes[probability_array_index]}\\x1b[0m with probability of: {_probability_value*100}%.\")\n",
    "\n",
    "            _some_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[probability_array_index] #get list of respective class\n",
    "            _some_services_embeddings[list(_unclassified_services_embeddings.keys())[list_index]] = _unclassified_services_embeddings.get(list(_unclassified_services_embeddings.keys())[list_index]) #map unclassified service to new list\n",
    "            del _Z[list_index] #remove from results\n",
    "            del _Z2[list_index]\n",
    "            del _unclassified_services_embeddings[list(_unclassified_services_embeddings.keys())[list_index]] #remove from unclassified\n",
    "            print(f\"left to classify: {len(_Z),len(_Z2)}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classes = ['persistentEntity', 'application', 'utility']\n",
    "\n",
    "file_app = open(\"../data/posapplication.txt\", \"r\")\n",
    "file_util = open(\"../data/posutility.txt\", \"r\")\n",
    "file_entity = open(\"../data/posentity.txt\", \"r\")\n",
    "\n",
    "app_text = file_app.read()\n",
    "entity_text = file_entity.read()\n",
    "util_text = file_util.read()\n",
    "\n",
    "_first_code_embeddings_list:list = list(embeddings_list_dict.values())[0]\n",
    "\n",
    "TP_util = 0\n",
    "TP_pers = 0\n",
    "TP_app = 0\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[2]#classified util\n",
    "_number_of_classified_util = len(_first_code_embeddings_list[2])\n",
    "_number_of_actual_util = util_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        TP_util = TP_util + 1\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[0]#classified pers\n",
    "_number_of_classified_pers = len(_first_code_embeddings_list[0])\n",
    "_number_of_actual_pers = entity_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        TP_pers = TP_pers + 1\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "\n",
    "_services: dict = _first_code_embeddings_list[1]#classified app\n",
    "_number_of_classified_app = len(_first_code_embeddings_list[1])\n",
    "_number_of_actual_app = app_text.count('\\n')\n",
    "for _out in _services.keys():\n",
    "    name = entities_dict[_out]['entityFullQualifiedName']\n",
    "    _err = True\n",
    "    if name in entity_text:\n",
    "        #print(f\"{name} is {classes[0]}.\")\n",
    "        _err = False\n",
    "    if name in app_text:\n",
    "        #print(f\"{name} is {classes[1]}.\")\n",
    "        TP_app = TP_app + 1\n",
    "        _err = False\n",
    "    if name in util_text:\n",
    "        #print(f\"{name} is {classes[2]}.\")\n",
    "        _err = False\n",
    "    if _err:\n",
    "        print(f\"{name} could not be found in truth files.\")\n",
    "if TP_pers + TP_app + TP_util > 0:\n",
    "    FP_util = _number_of_classified_util - TP_util\n",
    "    FN_util = _number_of_actual_util - TP_util\n",
    "    TN_util =  _number_of_actual_app + _number_of_actual_pers - FP_util\n",
    "    FP_pers = _number_of_classified_pers - TP_pers\n",
    "    FN_pers = _number_of_actual_pers - TP_pers\n",
    "    TN_pers = _number_of_actual_app + _number_of_actual_util - FP_pers\n",
    "    FP_app = _number_of_classified_app - TP_app\n",
    "    FN_app = _number_of_actual_app - TP_app\n",
    "    TN_app = _number_of_actual_util + _number_of_actual_pers - FP_app\n",
    "    _accuracy = (TP_util + TP_app + TP_pers + TN_app + TN_util + TN_pers) / (TP_util + TP_app + TP_pers + TN_app + TN_util + TN_pers + FP_app + FP_pers + FP_util + FN_util + FN_app + FN_pers)\n",
    "    print(f\"accuracy: {_accuracy}\")\n",
    "    _util_precision = TP_util / (TP_util + FP_util)\n",
    "    _util_recall = TP_util / (TP_util + FN_util)\n",
    "    print(f\"precision util: {_util_precision} | recall util: {_util_recall} | F-1 measure: {2 * ((_util_precision * _util_recall) / (_util_precision + _util_recall))}\")\n",
    "    _app_precision = TP_app / (TP_app + FP_app)\n",
    "    _app_recall = TP_app / (TP_app + FN_app)\n",
    "    print(f\"precision application: {_app_precision} | recall application: {_app_recall} | F-1 measure: {2 * ((_app_precision * _app_recall) / (_app_precision + _app_recall))}\")\n",
    "    _pers_precision = TP_pers / (TP_pers + FP_pers)\n",
    "    _pers_recall = TP_pers / (TP_pers + FN_pers)\n",
    "    print(f\"precision persistent entity: {_pers_precision} | recall application: {_pers_recall} | F-1 measure: {2 * ((_pers_precision * _pers_recall) / (_pers_precision + _pers_recall))}\")\n",
    "    _total_precision = (TP_util + TP_app + TP_pers) / (TP_util + TP_app + TP_pers + FP_app + FP_util + FP_pers)\n",
    "    _total_recall = (TP_util + TP_app + TP_pers) / (TP_util + TP_app + TP_pers + FN_app + FN_util + FN_pers)\n",
    "    print(f\"precision total: {_total_precision} | recall total: {_total_recall} | F-1 measure total: {2 * ((_total_precision * _total_recall) / (_total_precision + _total_recall))}\")\n",
    "else: print(\"Could not get truth.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for _embedding_list_key in embeddings_list_dict.keys():\n",
    "    _persistentEntity_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[0]    #pers\n",
    "    _application_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[1]        #app\n",
    "    _utility_services_embeddings = embeddings_list_dict.get(_embedding_list_key)[2]        #util\n",
    "    #print(embeddings_list_dict.get(_embedding_list_key)[3]) #unclassed\n",
    "    for _entity in _persistentEntity_services_embeddings.keys():\n",
    "        if not _entity in persistentEntity_services:\n",
    "            persistentEntity_services.add(_entity)\n",
    "            print(f\"pers {persistentEntity_services}\")\n",
    "    for _entity in _application_services_embeddings.keys():\n",
    "        if not _entity in application_services:\n",
    "            application_services.add(_entity)\n",
    "            print(f\"app {application_services}\")\n",
    "    for _entity in _utility_services_embeddings.keys():\n",
    "        if not _entity in utility_services:\n",
    "            utility_services.add(_entity)\n",
    "            print(f\"util {utility_services}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "R = nx.MultiDiGraph()\n",
    "\n",
    "for entity in entities:\n",
    "    if entity['entityId'] in allowed_types_set:\n",
    "        if entity['entityId'] in persistentEntity_services:\n",
    "            color = \"#691313\"  #red\n",
    "            entity_service = \"pers\"\n",
    "        elif entity['entityId'] in utility_services:\n",
    "            color = \"#133569\"  #blue\n",
    "            entity_service = \"util\"\n",
    "        elif entity['entityId'] in application_services:\n",
    "            color = \"#13692a\"  #green\n",
    "            entity_service = \"app\"\n",
    "        else:\n",
    "            entity_service = \"undefined\"\n",
    "            print(f\"unexpected entity type {entity['entityId']}.\")\n",
    "        G.add_node(entity['entityId'], entity_service=entity_service, name=entity['entityFullQualifiedName'],\n",
    "                   color=color)\n",
    "R = G.copy()\n",
    "for relation in relations:\n",
    "    source_id = relation['from']\n",
    "    target_id = relation['to']\n",
    "    relation_type = relation['relationType']\n",
    "    old_weight = 0\n",
    "    try:\n",
    "        old_weight = G.edges[source_id, target_id]['weight']\n",
    "    except KeyError:\n",
    "        old_weight = 0\n",
    "\n",
    "    weight = 0\n",
    "    if relation_type == 'CALL':\n",
    "        weight = 5 + old_weight\n",
    "        R.add_edge(source_id, target_id, weight=5)\n",
    "    elif relation_type == 'INTERFACE':\n",
    "        weight = 100 + old_weight\n",
    "        R.add_edge(source_id, target_id, weight=100)\n",
    "    elif relation_type == 'INHERITANCE':\n",
    "        weight = 100 + old_weight\n",
    "        R.add_edge(source_id, target_id, weight=100)\n",
    "    elif relation_type == 'FIELD':\n",
    "        weight = 25 + old_weight\n",
    "        R.add_edge(source_id, target_id, weight=25)\n",
    "    else:\n",
    "        print(f\"unconsidered relation type {relation_type}.\")\n",
    "    if source_id in G.nodes and target_id in G.nodes and not weight == 0:  #filter unwanted nodes\n",
    "        G.add_edge(source_id, target_id, weight=weight)\n",
    "print(G)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node_id in G.nodes():\n",
    "    print(f\"{node_id:3d} {G.nodes[node_id]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_graph(_graph: nx.Graph,size=6,iter=50):\n",
    "    # Visualize the graph\n",
    "    plt.figure(figsize=(size, size))\n",
    "    pos = nx.spring_layout(_graph, seed=42, iterations=iter)\n",
    "    colors = nx.get_node_attributes(_graph, \"color\")\n",
    "    nx.draw_networkx_nodes(_graph, pos, node_color=colors.values(), node_size=500)\n",
    "    nx.draw_networkx_edges(_graph, pos, edge_color=\"grey\")\n",
    "    nx.draw_networkx_labels(_graph, pos, font_size=9, font_family=\"sans-serif\", font_color=\"#ffffff\")\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        _graph, pos, edge_labels={(u, v): d[\"weight\"] for u, v, d in _graph.edges(data=True)}\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_graph(G,16,200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_sublist_app: list = []\n",
    "_sublist_util: list = []\n",
    "_sublist_pers: list = []\n",
    "for node in G.nodes():\n",
    "    if G.nodes[node]['entity_service'] == 'app':\n",
    "        _sublist_app.append(node)\n",
    "    elif G.nodes[node]['entity_service'] == 'util':\n",
    "        _sublist_util.append(node)\n",
    "    elif G.nodes[node]['entity_service'] == 'pers':\n",
    "        _sublist_pers.append(node)\n",
    "    else:\n",
    "        print('unexpected node entity_service type!')\n",
    "\n",
    "subgraph_app = nx.subgraph(G, _sublist_app)\n",
    "print(subgraph_app)\n",
    "subgraph_util = nx.subgraph(G, _sublist_util)\n",
    "print(subgraph_util)\n",
    "subgraph_pers = nx.subgraph(G, _sublist_pers)\n",
    "print(subgraph_pers)\n",
    "sub_graphs: list = [subgraph_app, subgraph_pers, subgraph_util]\n",
    "#for graph in sub_graphs:\n",
    "#    visualize_graph(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "service_part_graph_list: list = []\n",
    "for sub_graph in sub_graphs:\n",
    "    if not len(sub_graph.edges) < 1:\n",
    "        groupings = nx.community.louvain_communities(sub_graph, seed=42)\n",
    "        for subset in groupings:\n",
    "            grouping_graph = nx.subgraph(sub_graph, subset)\n",
    "            for node_id in grouping_graph.nodes():\n",
    "                print(f\"{node_id:3d} {G.nodes[node_id]}\")\n",
    "            #visualize_graph(G_test)\n",
    "            service_part_graph_list.append(grouping_graph)\n",
    "    else:\n",
    "        for node_n in range(len(list(sub_graph.nodes))):\n",
    "            grouping_graph = nx.subgraph(sub_graph, list(sub_graph.nodes)[node_n])\n",
    "            for node_id in grouping_graph.nodes():\n",
    "                print(f\"{node_id:3d} {G.nodes[node_id]}\")\n",
    "            service_part_graph_list.append(grouping_graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_summ_graph: nx.Graph = nx.Graph()\n",
    "_appl = 1\n",
    "_util = 1\n",
    "_pers = 1\n",
    "for i, _service_graph in enumerate(service_part_graph_list):\n",
    "    name = ''\n",
    "    color = ''\n",
    "    contained_nodes: list = list(_service_graph.nodes.keys())\n",
    "    if 'app' in list(nx.get_node_attributes(_service_graph, 'entity_service').values()):\n",
    "        name = f'appl_{_appl}'\n",
    "        _appl = _appl + 1\n",
    "        color = \"#13692a\"  #green\n",
    "    elif 'pers' in list(nx.get_node_attributes(_service_graph, 'entity_service').values()):\n",
    "        name = f'pers_{_pers}'\n",
    "        _pers = _pers + 1\n",
    "        color = \"#691313\"  #red\n",
    "    elif 'util' in list(nx.get_node_attributes(_service_graph, 'entity_service').values()):\n",
    "        name = f'util_{_util}'\n",
    "        _util = _util + 1\n",
    "        color = \"#133569\"  #blue\n",
    "    else:\n",
    "        print(print('unexpected node entity_service type!'))\n",
    "    _inter_service_edges: list = []\n",
    "    for _node_id in _service_graph:\n",
    "        all_edges = G.edges\n",
    "        for edge in all_edges:\n",
    "            if _node_id == edge[0] and not edge[1] in _service_graph.nodes:\n",
    "                _inter_service_edges.append(edge)\n",
    "            elif _node_id == edge[1] and not edge[0] in _service_graph.nodes:\n",
    "                _inter_service_edges.append(edge)\n",
    "    new_summ_graph.add_node(i, name=name, color=color, contained_nodes=contained_nodes,\n",
    "                            inter_service_edges=_inter_service_edges)\n",
    "\n",
    "_inter_service_edges: set = set()\n",
    "for new_node in new_summ_graph.nodes():\n",
    "    _inter_service_edges.update(new_summ_graph.nodes.get(new_node)['inter_service_edges'])\n",
    "\n",
    "_dict_old_to_new: dict = {}\n",
    "for new_node in new_summ_graph.nodes():\n",
    "    for old_node in new_summ_graph.nodes.get(new_node)['contained_nodes']:\n",
    "        _dict_old_to_new[old_node] = new_node\n",
    "\n",
    "for _inter_service_edge in _inter_service_edges:\n",
    "    weight = G.edges.get(_inter_service_edge)['weight']\n",
    "    start_node = _dict_old_to_new.get(_inter_service_edge[0])\n",
    "    end_node = _dict_old_to_new.get(_inter_service_edge[1])\n",
    "    try:\n",
    "        old_weight = new_summ_graph.edges[start_node, end_node]['weight']\n",
    "    except KeyError:\n",
    "        old_weight = 0\n",
    "    weight = weight + old_weight\n",
    "    new_summ_graph.add_edge(start_node, end_node, weight=weight)\n",
    "\n",
    "print(new_summ_graph)\n",
    "visualize_graph(new_summ_graph,16,200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "heaviest = 0\n",
    "for edge in new_summ_graph.edges():\n",
    "    _weight = new_summ_graph.edges.get(edge)['weight']\n",
    "    if _weight > heaviest:\n",
    "        heaviest = _weight\n",
    "static_weight_dict: dict = {}\n",
    "for edge in new_summ_graph.edges():\n",
    "    static_weight_dict[edge] = round(new_summ_graph.edges.get(edge)['weight'] / heaviest, 5)\n",
    "print(static_weight_dict)\n",
    "print(heaviest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sorted_text_dict_dict: dict = {}\n",
    "\n",
    "\n",
    "def get_words_for_cluster(node_id: int, graph: nx.Graph, filter_list: list) -> dict:\n",
    "    _cluster_text: str = ''\n",
    "    _code_for_class:int = 24775 #00110000011000111\n",
    "    _code_for_method:int = 218  #000011011000\n",
    "\n",
    "    \"\"\"METHOD VALUES\n",
    "    annotations = 0\n",
    "    annotation_arguments = 1\n",
    "    method_head = 2\n",
    "    method_modifiers = 3\n",
    "    return_types = 4\n",
    "    method_parameters = 5\n",
    "    parameter_annotations = 6\n",
    "    parameter_types = 7\n",
    "    parameter_names = 8\n",
    "    method_body = 9\n",
    "    comments = 10\n",
    "    method_java_doc = 11\n",
    "    \"\"\"\n",
    "\n",
    "    for contained_node in graph.nodes.get(node_id)['contained_nodes']:\n",
    "        _cluster_text = _cluster_text + \" \" + get_class(entities_dict[contained_node],\n",
    "                                                  inherited_methods=get_position_invert(_code_for_class, 0),\n",
    "                                                  package=get_position_invert(_code_for_class, 1),\n",
    "                                                  imports=get_position_invert(_code_for_class, 2),\n",
    "                                                  comments=get_position_invert(_code_for_class, 3),\n",
    "                                                  java_doc=get_position_invert(_code_for_class, 4),\n",
    "                                                  instance_head=get_position_invert(_code_for_class, 5),\n",
    "                                                  instance_modifiers=get_position_invert(_code_for_class, 6),\n",
    "                                                  instance_type=get_position_invert(_code_for_class, 7),\n",
    "                                                  instance_name=get_position_invert(_code_for_class, 8),\n",
    "                                                  instance_extension=get_position_invert(_code_for_class, 9),\n",
    "                                                  instance_implementations=get_position_invert(_code_for_class, 10),\n",
    "                                                  fields=get_position_invert(_code_for_class, 11),\n",
    "                                                  field_annotations=get_position_invert(_code_for_class, 12),\n",
    "                                                  field_modifiers=get_position_invert(_code_for_class, 13),\n",
    "                                                  field_type=get_position_invert(_code_for_class, 14),\n",
    "                                                  field_name=get_position_invert(_code_for_class, 15),\n",
    "                                                  methods=get_position_invert(_code_for_class, 16),\n",
    "                                                  method_code=_code_for_method)\n",
    "    _cluster_text = re.sub(r'[^A-Za-z ]+', ' ', _cluster_text)\n",
    "    content_list = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', _cluster_text)).split()\n",
    "    s: set = set(filter_list)\n",
    "    content_list = [x.lower() for x in content_list]\n",
    "    temp3 = [x for x in content_list if x not in s]\n",
    "    my_dict = {i: temp3.count(i) for i in temp3}\n",
    "    keys = list(my_dict.keys())\n",
    "    values = list(my_dict.values())\n",
    "    sorted_value_index = np.argsort(values)[::-1][:]\n",
    "    return {keys[i]: values[i] for i in sorted_value_index}\n",
    "\n",
    "\n",
    "filter_list: list = []\n",
    "with open(\"../data/java_keywords.txt\", \"r\") as file_entity:\n",
    "    for line in file_entity:\n",
    "        filter_list.append(line.rstrip())\n",
    "\n",
    "for node_id in new_summ_graph.nodes():\n",
    "    sorted_text_dict_dict[node_id] = get_words_for_cluster(node_id, new_summ_graph, filter_list)\n",
    "    print(get_words_for_cluster(node_id, new_summ_graph, filter_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "import gensim.downloader\n",
    "\n",
    "google_model = gensim.downloader.load('word2vec-google-news-300')\n",
    "weighted_service_average_vectors_dict: dict = {}\n",
    "for node in new_summ_graph.nodes():\n",
    "    vec = np.zeros((300,))\n",
    "    summ_of_weights = 0\n",
    "    for word in list(sorted_text_dict_dict.get(node).keys()):\n",
    "        try:\n",
    "            multiplicity = sorted_text_dict_dict.get(node).get(word)\n",
    "            #if word in _persistent_word_dict:\n",
    "            #    multiplicity = multiplicity*_persistent_word_dict.get(word)*2\n",
    "            vec:ndarray = ((google_model[word] * multiplicity) + (vec * summ_of_weights)) / (multiplicity + summ_of_weights)\n",
    "            summ_of_weights = summ_of_weights + multiplicity\n",
    "        except KeyError:\n",
    "            print(f\"\\x1b[33m word not found: {word}\\x1b[0m\")\n",
    "            del sorted_text_dict_dict.get(node)[word]\n",
    "    weighted_service_average_vectors_dict[node] = vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "linguistic_cosine_similarity_edges: dict = {}\n",
    "_temp_done_list: list = []\n",
    "for edge in new_summ_graph.edges():\n",
    "    distance = 1 - spatial.distance.cosine(weighted_service_average_vectors_dict.get(edge[0]),\n",
    "                                           weighted_service_average_vectors_dict.get(edge[1]))\n",
    "    print(f\"for {edge[0]} and {edge[1]} the cosine lingual distance is {distance} \")\n",
    "    linguistic_cosine_similarity_edges[edge] = distance\n",
    "#for node_1 in new_summ_graph.nodes():\n",
    "#    for node_2 in new_summ_graph.nodes():\n",
    "#        if not (node_1, node_2) in linguistic_cosine_similarity_edges.keys() and not (node_2, node_1) in linguistic_cosine_similarity_edges.keys() and not node_1 == node_2:\n",
    "#            distance = 1 - spatial.distance.cosine(weighted_service_average_vectors_dict.get(node_1), weighted_service_average_vectors_dict.get(node_2))\n",
    "#            print(f\"for {node_1} and {node_2} the cosine lingual distance is {distance} \")\n",
    "#            linguistic_cosine_similarity_edges[(node_1,node_2)] = distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "def drange(x, y, jump) -> list:\n",
    "    result: list = [float(x)]\n",
    "    x = decimal.Decimal(x)\n",
    "    while x < y:\n",
    "        result.append(round(float(x), 9))\n",
    "        x += decimal.Decimal(jump)\n",
    "    return result\n",
    "\n",
    "l = list(drange(0.0, 1.01, '0.01'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(linguistic_cosine_similarity_edges.values(), bins=l)\n",
    "plt.xlabel('w(i,j)')\n",
    "plt.ylabel('occurrence')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "centers: list = []\n",
    "for node in new_summ_graph.nodes():\n",
    "    if 'appl' in new_summ_graph.nodes.get(node)['name']:\n",
    "        centers.append(node)\n",
    "print(centers)\n",
    "A = nx.to_numpy_array(new_summ_graph)\n",
    "print(A.shape)\n",
    "print(A / heaviest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_weight_of_pair(i: int, j: int, alpha: float = 1.0, beta: float = 1.0) -> float:\n",
    "    weight_static = A[(i, j)] / heaviest\n",
    "    weight_lingual = linguistic_cosine_similarity_edges.get((i, j), 0) + linguistic_cosine_similarity_edges.get((j, i),\n",
    "                                                                                                                0)\n",
    "    if i == j:\n",
    "        return 1.0\n",
    "    return (alpha * weight_static) + (beta * weight_lingual) / (alpha + beta)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a: float = 0.7\n",
    "b: float = 0.3\n",
    "MS = np.zeros((len(new_summ_graph.nodes()), len(centers)))\n",
    "for center_id in centers:\n",
    "    for node_id in new_summ_graph.nodes():\n",
    "        if not get_weight_of_pair(center_id, node_id, a, b) == 0 and not 'app' in new_summ_graph.nodes.get(node_id)[\n",
    "            'name']:\n",
    "            summ: float = 0.0\n",
    "            for c in centers:\n",
    "                summ = summ + get_weight_of_pair(c, node_id, a, b)\n",
    "            MS[node_id, center_id] = round(pow((get_weight_of_pair(center_id, node_id, a, b) / summ), 2), 6)\n",
    "        else:\n",
    "            MS[node_id, center_id] = 0\n",
    "print(MS)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_services: dict = {}\n",
    "for center_id in centers:\n",
    "    _service_graph = nx.Graph()\n",
    "    _service_graph.add_node(center_id, name=new_summ_graph.nodes.get(center_id)['name'],\n",
    "                            #add center application cluster\n",
    "                            color=new_summ_graph.nodes.get(center_id)['color'],\n",
    "                            contained_nodes=new_summ_graph.nodes.get(center_id)['contained_nodes'],\n",
    "                            inter_service_edges=new_summ_graph.nodes.get(center_id)['inter_service_edges'])\n",
    "    for node_id in new_summ_graph.nodes():\n",
    "        if MS[node_id, center_id] > 0.02:\n",
    "            _service_graph.add_node(node_id, name=new_summ_graph.nodes.get(node_id)['name'],\n",
    "                                    color=new_summ_graph.nodes.get(node_id)['color'],\n",
    "                                    contained_nodes=new_summ_graph.nodes.get(node_id)['contained_nodes'],\n",
    "                                    inter_service_edges=new_summ_graph.nodes.get(node_id)['inter_service_edges'])\n",
    "            _service_graph.add_edge(node_id, center_id, weight=MS[node_id, center_id])\n",
    "    final_services[new_summ_graph.nodes.get(center_id)['name']] = _service_graph\n",
    "all_classes_used: dict = {}\n",
    "for o,i in enumerate(final_services.values()):\n",
    "    print(f\"{o}.\")\n",
    "    for c in i.nodes():\n",
    "        print(c,i.nodes.get(c)['name'])\n",
    "        for n in new_summ_graph.nodes.get(c)['contained_nodes']:\n",
    "            print(G.nodes.get(n)['name'])\n",
    "            try:\n",
    "                all_classes_used[n] = all_classes_used[n] + 1\n",
    "            except KeyError:\n",
    "                all_classes_used[n] = 1\n",
    "    visualize_graph(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "multiple_used_aplication: list = []\n",
    "for i in allowed_types_set:\n",
    "    try:\n",
    "        if all_classes_used[i] > 1 and i in application_services:\n",
    "            print(f\"\\x1b[33m{G.nodes.get(i)['name']} was used in {all_classes_used[i]} services.\\x1b[0m\")\n",
    "            multiple_used_aplication.append(i)\n",
    "        else:\n",
    "            if G.nodes.get(i)['entity_service'] == \"util\":\n",
    "                print(f\"\\x1b[34m{G.nodes.get(i)['name']}\\x1b[0m was used in {all_classes_used[i]} services.\")\n",
    "            elif G.nodes.get(i)['entity_service'] == \"pers\":\n",
    "                print(f\"\\x1b[31m{G.nodes.get(i)['name']}\\x1b[0m was used in {all_classes_used[i]} services.\")\n",
    "            elif G.nodes.get(i)['entity_service'] == \"app\":\n",
    "                print(f\"\\x1b[32m{G.nodes.get(i)['name']}\\x1b[0m was used in {all_classes_used[i]} services.\")\n",
    "            else:\n",
    "                print(f\"{G.nodes.get(i)['name']} was used in {all_classes_used[i]} services.\")\n",
    "    except KeyError:\n",
    "        _numbers = G.edges(i)\n",
    "        _edges: list = []\n",
    "        for c in _numbers:\n",
    "            _edges.append(G.edges.get(c))\n",
    "        _weight = sum(x[\"weight\"] for x in _edges)\n",
    "        print(\n",
    "            f\"\\x1b[31m\\033[01m\\033[04m{G.nodes.get(i)['name']} was not used in any service and has {len(_edges)} edges originally with a weight {_weight} !\\x1b[0m\")  #print it red"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from networkx import NetworkXError, MultiDiGraph\n",
    "yellow = \"#B2B200\"\n",
    "_resulting_ms_graph:MultiDiGraph = R.copy()\n",
    "_final_services_detailed:dict = {}\n",
    "_internal_weights:dict = {}\n",
    "_external_weights:dict = {}\n",
    "_rename_mappings:list = []\n",
    "_ms_sets:list = []\n",
    "_used_node_from_outer_scope:dict = {}\n",
    "for o,i in enumerate(final_services.values()):\n",
    "    print(f\"Service Candidate: {o}.\")\n",
    "    _new_ms_candidate_set = set()\n",
    "    _new_ms_names_set = set()\n",
    "    _used_node_from_outer_scope[o] = 0\n",
    "    _reference_node = None\n",
    "    for c in i.nodes():\n",
    "        print(c,i.nodes.get(c)['name'])\n",
    "        if 'appl' in i.nodes.get(c)['name']:\n",
    "            _reference_node = new_summ_graph.nodes.get(c)['contained_nodes'][0]\n",
    "        for n in new_summ_graph.nodes.get(c)['contained_nodes']:\n",
    "            print(R.nodes.get(n)['name'])\n",
    "            _new_ms_candidate_set.add(n)\n",
    "            _new_ms_names_set.add(G.nodes.get(n)['name'])\n",
    "    #collecting incoming edges to different nodes\n",
    "    R:MultiDiGraph = nx.MultiDiGraph(R)\n",
    "    _already_exposed:set = set()\n",
    "    for _inner_node in _new_ms_candidate_set:\n",
    "        for _in_edge in R.in_edges(_inner_node):\n",
    "            if not _inner_node in _already_exposed and not set(_in_edge).difference(_new_ms_candidate_set) == set() and 'javafx.fxml.Initializable' in entities_dict.get(_inner_node)['interfaces']:\n",
    "                _used_node_from_outer_scope[o] = _used_node_from_outer_scope[o] + 1\n",
    "                _already_exposed.add(_inner_node)\n",
    "\n",
    "    _ms_sets.append(_new_ms_names_set.copy())\n",
    "    _temp_copy_graph = nx.subgraph(R, _new_ms_candidate_set)\n",
    "    _temp_copy_graph = nx.MultiDiGraph(_temp_copy_graph)\n",
    "    _temp_copy_graph.remove_edges_from(nx.selfloop_edges(_temp_copy_graph))\n",
    "    _final_services_detailed[o] = _temp_copy_graph\n",
    "    visualize_graph(_final_services_detailed.get(o),8,50)\n",
    "    _internal_weights[f\"MS{o}\"] = _final_services_detailed.get(o).size(weight=\"weight\")\n",
    "    _new_ms_candidate_set.remove(_reference_node)\n",
    "    while len(_new_ms_candidate_set) >= 1:\n",
    "        try:\n",
    "            _resulting_ms_graph = nx.contracted_nodes(_resulting_ms_graph, _reference_node, _new_ms_candidate_set.pop())\n",
    "        except NetworkXError:\n",
    "            continue\n",
    "    _resulting_ms_graph.nodes.get(_reference_node)['color'] = yellow\n",
    "    _rename_mappings.append({_reference_node : f\"MS{o}\"})\n",
    "\n",
    "while len(_rename_mappings) >= 1:\n",
    "    _resulting_ms_graph = nx.relabel_nodes(_resulting_ms_graph, _rename_mappings.pop())\n",
    "_resulting_ms_graph.remove_edges_from(nx.selfloop_edges(_resulting_ms_graph))\n",
    "_resulting_ms_graph:MultiDiGraph = MultiDiGraph(_resulting_ms_graph)\n",
    "\n",
    "print(f\"Representative call graph for all forgotten nodes and ms candidates (yellow).\")\n",
    "visualize_graph(_resulting_ms_graph,10,80)\n",
    "\n",
    "#recall&precision\n",
    "file_MS_GT = open(\"../data/service_candidates_GT.txt\", \"r\")\n",
    "_bounded_sets:list = []\n",
    "_set_mappings:dict = {}\n",
    "_ms_recall:dict = {}\n",
    "_ms_precision:dict = {}\n",
    "for line in file_MS_GT:\n",
    "    _bounded_context_list:list = line.strip('\\n').split(',')\n",
    "    _bounded_sets.append(set(_bounded_context_list))\n",
    "for s,_ms in enumerate(_ms_sets):\n",
    "    _max_value = 0\n",
    "    for b,_bs in enumerate(_bounded_sets):\n",
    "        if len(_bs)/(len(_bs)+len(set(_bs).difference(_ms)))> _max_value:\n",
    "            _max_value = len(_bs)/(len(_bs)+len(set(_bs).difference(_ms)))\n",
    "            _set_mappings[s] = b\n",
    "    _ms_recall[s] = _max_value\n",
    "#print(_set_mappings)\n",
    "for map_key in _set_mappings.keys():\n",
    "    _ms_precision[map_key] = (len(_bounded_sets[_set_mappings.get(map_key)])/len(set(_bounded_sets[_set_mappings.get(map_key)]).union(_ms_sets[map_key])))\n",
    "\n",
    "_average_precision = sum(_ms_precision.values())/len(_ms_precision)\n",
    "_average_recall = sum(_ms_recall.values())/len(_ms_recall)\n",
    "_more_ms = len(_ms_sets)-len(_bounded_sets)\n",
    "_multi_uses = list(filter(lambda x: (x > 1),Counter(_set_mappings.values()).values()))\n",
    "_actual_multiple_uses = sum(_multi_uses)-len(_multi_uses)\n",
    "_non_used_bs = len(_bounded_sets)-len(Counter(_set_mappings.values()))\n",
    "#for map in _set_mappings.keys():\n",
    "#    print(map,_ms_recall.get(map)+_ms_precision.get(map))\n",
    "\n",
    "\n",
    "#gathering external weigths\n",
    "for _node in _resulting_ms_graph.nodes():\n",
    "    _external_weights[_node] = float(_resulting_ms_graph.out_degree(_node, weight=\"weight\"))\n",
    "#print(_internal_weights,_external_weights)\n",
    "_list_of_independent_nodes = list(filter(lambda e: 'MS' not in str(e),_resulting_ms_graph.nodes()))\n",
    "_list_of_independent_interfaces = list(filter(lambda e: 'javafx.fxml.Initializable' in entities_dict.get(e)['interfaces'],_list_of_independent_nodes))\n",
    "print(f\"IFN: {round((sum(_used_node_from_outer_scope.values())+len(_list_of_independent_interfaces)) / (len(_used_node_from_outer_scope.values())+len(_list_of_independent_nodes)),4)}\")\n",
    "_cohesion_dict:dict = {}\n",
    "for inner_key in _internal_weights.keys():\n",
    "    #print(_external_weights.get(inner_key),_internal_weights.get(inner_key))\n",
    "    _cohesion_dict[inner_key] = _internal_weights.get(inner_key)/_external_weights.get(inner_key)\n",
    "#print(_cohesion_dict)\n",
    "print(f\"average cohesion is: {round(sum(list(_cohesion_dict.values()))/len(list(_cohesion_dict.values())),4)}\")\n",
    "print(f\"average coupling is: {round(sum(list(_external_weights.values()))/len(list(_external_weights.values())),4)}\")\n",
    "print(f\"average precision is: {_average_precision} compared to Ground Truth services.\")\n",
    "print(f\"average recall is: {_average_recall} compared to Ground Truth services.\")\n",
    "print(f\"there were: {_more_ms} more microservice candidates identified than anticipated.\\n\"\n",
    "      f\"{len(_multi_uses)} Ground Truth services were found {_actual_multiple_uses} times too often, and {_non_used_bs} Ground Truth services were not found.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
